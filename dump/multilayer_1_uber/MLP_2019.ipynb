{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Unnormalized Data \n",
      "     CO2_Zone_1   CO2_Zone_2   CO2_Zone_3   CO2_Zone_4   CO2_Zone_5  \\\n",
      "0   582.242991   815.371584   573.770973   736.007364   533.105710   \n",
      "1  1285.206632  1262.129709  1459.823383  1430.411951  1335.115158   \n",
      "2   483.022072   635.978451   477.837200   607.979247   452.949922   \n",
      "3  1666.038626  1700.412835  1666.040031  1683.226148  1674.632006   \n",
      "4  1263.113447  1576.165384  1388.204880  1278.984458  1320.229164   \n",
      "\n",
      "    CO2_Zone_6  pas_cnt  \n",
      "0   690.753919       47  \n",
      "1  1225.653504       93  \n",
      "2   564.857383       47  \n",
      "3  1694.683670      186  \n",
      "4  1377.015714      140   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD,Adam\n",
    "import os.path\n",
    "import json\n",
    "from keras.models import model_from_json\n",
    "\n",
    "# For reproducibility - splitting train and test sets\n",
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "\n",
    "\n",
    "# Load data from Excel sheets\n",
    "\n",
    "dataset1 = pd.read_excel('Simulink_Data_25.xlsx')\n",
    "dataset2 = pd.read_excel('Simulink_Data_25_1.xlsx')\n",
    "dataset3 = pd.read_excel('Simulink_Data_50.xlsx')\n",
    "dataset4 = pd.read_excel('Simulink_Data_50_1.xlsx')\n",
    "dataset5 = pd.read_excel('Simulink_Data_75.xlsx')\n",
    "dataset6 = pd.read_excel('Simulink_Data_75_1.xlsx')\n",
    "dataset7 = pd.read_excel('Simulink_Data_100.xlsx')\n",
    "dataset8 = pd.read_excel('Simulink_Data_100_1.xlsx')\n",
    "#Combine datasets into one single data file\n",
    "frames=[dataset1,dataset2,dataset3,dataset4,dataset5,dataset6,dataset7,dataset8]\n",
    "dataset = pd.concat(frames)\n",
    "\n",
    "#  Shuffle Data\n",
    "dataset = dataset.sample(frac=1).reset_index(drop=True)\n",
    "X_complete=dataset.drop('class',axis=1)\n",
    "y_complete=dataset['class']\n",
    "\n",
    "print(type(X_complete))\n",
    "print(\"Unnormalized Data\", \"\\n\", X_complete[:5], \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized Data\n",
      "    CO2_Zone_1  CO2_Zone_2  CO2_Zone_3  CO2_Zone_4  CO2_Zone_5  CO2_Zone_6  \\\n",
      "0   -1.088624   -0.950637   -1.235218   -1.170648   -1.368075   -1.208656   \n",
      "1    0.274130   -0.022274    0.525111    0.329783    0.224104   -0.066552   \n",
      "2   -1.280972   -1.323416   -1.425811   -1.447284   -1.527203   -1.477467   \n",
      "3    1.012405    0.888478    0.934804    0.876050    0.898126    0.934910   \n",
      "4    0.231301    0.630292    0.382826    0.002587    0.194552    0.256633   \n",
      "\n",
      "    pas_cnt  \n",
      "0 -1.339693  \n",
      "1 -0.452990  \n",
      "2 -1.339693  \n",
      "3  1.339693  \n",
      "4  0.452990   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Feature scaling according to training set data \n",
    "col=list(X_complete.columns.values)\n",
    "\n",
    "for i in col:\n",
    "    avg=X_complete[str(i)].mean()\n",
    "    sd=X_complete[str(i)].std()\n",
    "    X_complete[str(i)]=X_complete[str(i)].apply(lambda X:(X-avg)/(sd))\n",
    "\n",
    "print(\"Normalized Data\\n\", X_complete[:5], \"\\n\")\n",
    "\n",
    "\n",
    "# covert to array for processing\n",
    "X_complete=X_complete.values\n",
    "\n",
    "#One hot encoding\n",
    "y_complete = pd.get_dummies(y_complete).values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model weights data not found. Model will be fit on training set now.\n",
      "Train on 60485 samples, validate on 25923 samples\n",
      "Epoch 1/10\n",
      "60485/60485 [==============================] - 2s 25us/step - loss: 0.1920 - acc: 0.9223 - val_loss: 0.1389 - val_acc: 0.9480\n",
      "Epoch 2/10\n",
      "60485/60485 [==============================] - 1s 12us/step - loss: 0.1405 - acc: 0.9438 - val_loss: 0.1508 - val_acc: 0.9284\n",
      "Epoch 3/10\n",
      "60485/60485 [==============================] - 1s 11us/step - loss: 0.1359 - acc: 0.9464 - val_loss: 0.1435 - val_acc: 0.9478\n",
      "Epoch 4/10\n",
      "60485/60485 [==============================] - 1s 11us/step - loss: 0.1311 - acc: 0.9485 - val_loss: 0.1229 - val_acc: 0.9523\n",
      "Epoch 5/10\n",
      "60485/60485 [==============================] - 1s 12us/step - loss: 0.1318 - acc: 0.9498 - val_loss: 0.1203 - val_acc: 0.9516\n",
      "Epoch 6/10\n",
      "60485/60485 [==============================] - 1s 11us/step - loss: 0.1259 - acc: 0.9520 - val_loss: 0.1218 - val_acc: 0.9539\n",
      "Epoch 7/10\n",
      "60485/60485 [==============================] - 1s 12us/step - loss: 0.1178 - acc: 0.9570 - val_loss: 0.1049 - val_acc: 0.9616\n",
      "Epoch 8/10\n",
      "60485/60485 [==============================] - 1s 13us/step - loss: 0.1085 - acc: 0.9614 - val_loss: 0.0967 - val_acc: 0.9668\n",
      "Epoch 9/10\n",
      "60485/60485 [==============================] - 1s 13us/step - loss: 0.0972 - acc: 0.9646 - val_loss: 0.0997 - val_acc: 0.9666\n",
      "Epoch 10/10\n",
      "60485/60485 [==============================] - 1s 12us/step - loss: 0.0941 - acc: 0.9660 - val_loss: 0.0789 - val_acc: 0.9713\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_62 (Dense)             (None, 20)                160       \n",
      "_________________________________________________________________\n",
      "dense_63 (Dense)             (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dense_64 (Dense)             (None, 2)                 42        \n",
      "=================================================================\n",
      "Total params: 622\n",
      "Trainable params: 622\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Creating a Train and a Test Dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_complete, y_complete, test_size=0.3, random_state=seed)\n",
    "\n",
    "\n",
    "# Define Neural Network model layers\n",
    "model = Sequential()\n",
    "model.add(Dense(20, input_dim=7, activation='relu'))\n",
    "model.add(Dense(20, input_dim=7, activation='relu'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "# Compile model\n",
    "model.compile(Adam(lr=0.01),'categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "\n",
    "#ttt=1\n",
    "\n",
    "if os.path.isfile('#MLP_CO2_2019_weights.h5'):\n",
    "\n",
    "    # Model reconstruction from JSON file\n",
    "    json_file = open('MLP_CO2_2019_architecture.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    model = model_from_json(loaded_model_json)\n",
    "    \n",
    "    # Load weights into the new model\n",
    "    model.load_weights('MLP_CO2_2019_weights.h5')\n",
    "    print(\"Model weights loaded from saved model data.\")\n",
    "\n",
    "    model.compile(Adam(lr=0.01),'categorical_crossentropy',metrics=['accuracy'])\n",
    "else:\n",
    "#if ttt==1:\n",
    "    print(\"Model weights data not found. Model will be fit on training set now.\")\n",
    "\n",
    "    # Fit model on training data - try to replicate the normal input\n",
    "    model.fit(X_train,y_train,epochs=10,batch_size=256,verbose=1,validation_data=(X_test,y_test))\n",
    "    \n",
    "    # Save parameters to JSON file\n",
    "    model_json = model.to_json()\n",
    "    with open(\"MLP_2019_architecture.json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "\n",
    "    # Save model weights to file\n",
    "    model.save_weights('MLP_CO2_2019_weights.h5')\n",
    "\n",
    "\n",
    "model.summary()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 0 ... 0 1 1] [1 1 0 ... 0 1 1]\n",
      "25923/25923 [==============================] - 0s 6us/step\n",
      "[0.1068155754390485, 0.9590710952308348]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      0.98      0.97     20202\n",
      "          1       0.94      0.87      0.90      5721\n",
      "\n",
      "avg / total       0.96      0.96      0.96     25923\n",
      "\n",
      "[[19886   316]\n",
      " [  745  4976]]\n"
     ]
    }
   ],
   "source": [
    "# Model predictions for test set\n",
    "y_pred = model.predict(X_test)\n",
    "y_test_class = np.argmax(y_test,axis=1)\n",
    "y_pred_class = np.argmax(y_pred,axis=1)\n",
    "\n",
    "\n",
    "print(y_test_class,y_pred_class)\n",
    "\n",
    "# Evaluate model on test data\n",
    "score = model.evaluate(X_test, y_test, batch_size=128,verbose=1)\n",
    "print(score)\n",
    "\n",
    "# Compute stats on the test set and Output all results\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "print(classification_report(y_test_class,y_pred_class))\n",
    "print(confusion_matrix(y_test_class,y_pred_class))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    " model.load_weights('MLP_CO2_2019_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    " model.save_weights('MLP_CO2_2019_weights.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
